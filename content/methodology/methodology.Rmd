---
title: "Local Government Transparency Scores: Methodology"
author:
  - name: F. Daniel Hidalgo 
    url: https://www.dhidalgo.me
    affiliation: MIT
    affiliation_url: https://www.mit.edu
date: "2019-06-29"
bibliography: citations.bib
output: distill::distill_article
---

```{r setup, include=FALSE}
library(tidyverse); library(kableExtra)
library(drake); library(statebins)
library(mlr); library(patchwork)
loadd("bdg_errors")
loadd("min_errors")
loadd("rec_errors")
loadd("agd_errors")
loadd("bid_errors")
loadd("cafr_errors")
loadd("text_traindf")
loadd("urls")
loadd("labels")
```

This document describes the methodology used to create the transparency
indicators for the Government Transparency Project. We use webscraping and machine learning models to estimate whether a given website has the following transparency indicators:

- **Meeting Agendas (*AGD*)**: Posted agendas for regular meetings of the main policy-making body (e.g. the city council or its equivalent) of the local government.
- **Budgets (*BDG*)**: Town budget. 
- **Public Bids (*BID*)**: Posted information about bids for contracts to provide goods or services for the city. 
- **Comprehensive annual financial reports (*CAFR*)**: Posted annual financial reports. 
- **Meeting Minutes (*MIN*)**: Posted minutes for regular meetings of the main policy-making body (e.g. the city council or its equivalent) of the local government.
- **Public Records Requests Information (*REC*)**: Description or form that states how to obtain public records.

Below, we describe the specific data and algorithms used to generate the indicators included on the transparency report cards.

# Data

## Training Sample

```{r training_sample, echo = FALSE, warning = FALSE, message = FALSE}
num_training <- length(unique(labels$ST_FIPS))
census <- read_csv("../../../data/covariates/census_ST_FIPS.csv") %>%
  select(STATE, NAME, STNAME, CENSUS2010POP, ST_FIPS) %>%
  arrange(STNAME, NAME) %>%
  distinct()
census$CENSUS2010POP <- as.numeric(census$CENSUS2010POP)
census$NAME <-  str_remove(census$NAME, " \\(pt\\.\\)| \\(balance\\)|Balance of") %>%
  str_trim()
census$ST_FIPS <- as.numeric(census$ST_FIPS)
census <- group_by(census, STNAME, ST_FIPS, NAME) %>% 
  filter(grepl("County", NAME) == FALSE & NAME != STNAME) %>%
  summarise(pop = sum(CENSUS2010POP)) %>%
  arrange(desc(pop)) %>%
  mutate(training_sample = ifelse(ST_FIPS %in% labels$ST_FIPS, "Training Sample",
                                  "Not in Training Sample"))
census_training <- filter(census, training_sample == "Training Sample")

``` 

The sampling frame for our training sample are all US towns with a population
greater than 1000, as measured by the 2010 census. Specifically, we use census
data on "incorporated places" as our target population, which amounts to `r nrow(census)`
administrative units.

Our training sample consists of manually labeled indicators from `r num_training` towns. The sample is broadly representative of the population of
towns with populations greater than 1000 in the United States, with some
over-representation of particular states such as New York and Texas.^[These
states were over-sampled because external validation data exists for towns in
these states. This external validation data will be used to assess the overall
validity of the transparency score.] Because of the over-representation of towns
from a few states, the typical town in our training sample is larger in
population than the typical town in the US, but the distributions overlap
substantially. This comparison is visualized in Figure
\@ref(fig:trainingSampleHist).

```{r trainingSampleHist, echo = FALSE, warning = FALSE,  fig.cap = 'Distribution of Town Population in Training Sample and All US Towns with population greather than 1000.'}
census %>%
  mutate(training_sample = "All Towns") %>%
  bind_rows(census_training) %>%
  filter(pop >= 1000) %>%
  ggplot(data = ., aes(x = pop, fill = training_sample)) +
  geom_density(alpha = .5) +
  scale_x_log10(label = scales::comma) +
  xlab("Population") + 
  cowplot::theme_cowplot() +
  theme(legend.position = "top", legend.title = element_blank())
```


The training sample towns represent `r length(unique(census_training$STNAME))`
distinct states. The geographic distribution of the training sample by state is depicted in Figure \@ref(fig:trainingSampleMap).

```{r trainingSampleMap, echo=FALSE, warning=FALSE, fig.cap = "Geographic Distribution of Towns in the Training Sample"}
group_by(census_training, STNAME) %>%
  summarise( n =  n()) %>%
  statebins(state_col = "STNAME", name = "# of Towns",
            value_col = "n", direction = 1) +
  theme_statebins()
```

### Scraping Websites

To obtain the text data used to train the model or to label a website, we use
the Python crawling framework [Scrapy](https://scrapy.org). Specifically, we
crawl each website up to a depth of 2, with a maximum limit of 500 pages, and
save the HTML of each crawled page. This depth limit means that in addition to the
home page (level 0), we crawl pages that are linked from the home page (level 1), and pages that are
linked from those pages as well (level 2). Finally, we impose a limit of 500 pages to keep
the data of manageable size. 

### Creating a Document Term Matrix

For use in the classification model, we concatenate selected text from all pages
of a given website and transform the text into a document term matrix (DTM).
Specifically, from each scraped page we extract the (1) text in the title of the
page, (2) the text in all links of each page, and (3) the full text of the page
    itself. Each set of text (i.e. title text, page text, or link text) is
concatenated into a separate text string.

We then transform and pre-process each string of text into a document term
matrix using the *R* package *Quanteda*[@quanteda]. Specifically, we remove
punctuation, symbols, hyphens, and a list of common stop words. After this
pre-processing, we convert the string to a document-term matrix of unigram counts.
When creating the training set, we remove any token that does not occur in more
than 10% of the websites.

After pre-processing and conversion is completed, the title text DTM,
the link text DTM, and the page text DTM are combined into a single matrix,
though columns denoting the term counts from each of the tree text types (links,
titles, and pages) are kept in separate columns. The DTM generated from our
training sample using these procedures has `r ncol(text_traindf)` columns.

# Classification Model

To classify websites we use a *random forest*[@breiman2001random] model combined
with a feature selection pre-processing step. The feature selection step reduces
the dimensionality of the of the full DTM considerably and improves the
predictive performance of the random forest model.

### Feature Selection


```{r filter, echo = FALSE}
loadd("bdg_mod", "agd_mod", "bid_mod", "cafr_mod", "min_mod", "rec_mod")

```
The feature selection step relies on calculating a bivariate correlation between
the transparency indicator and each column of the document term matrix.
Specifically, we compute the *information gain* between the indicator and each
column of the document term matrix, which is a univariate measure of association
between the two variables.. We then rank the columns
with respect to information gain and keep the top $p$% percent of the variables,
where $p$ is chosen via cross-validation. 

The proportion of terms retained for each model are as follows:

- *AGD*:  `r round(agd_mod$tuned$par.vals$fw.perc, 2)`
- *BDG*: `r round(bdg_mod$tuned$par.vals$fw.perc, 2)`
- *BID*: `r round(bid_mod$tuned$par.vals$fw.perc, 2)`
- *CAFR*: `r round(cafr_mod$tuned$par.vals$fw.perc, 2)`
- *MIN*: `r round(min_mod$tuned$par.vals$fw.perc, 2)`
- *REC*: `r round(rec_mod$tuned$par.vals$fw.perc, 2)`

### Random Forest Model

After weakly correlated variables are pruned from the document term matrix via the feature selection step, we
use a Random Forest model to predict the transparency indicator. 
Specifically, we use the implementation in the ranger[@ranger] package, which is
optimized for speed. Each model is an ensemble of `r bdg_mod$trained$learner.model$next.model$learner.model$num.trees` different
classification trees. When constructing individual trees, Random Forest models
use only  a
random subset of both the training sample and the predictors (post-feature selection). Specifically,
each tree uses a random sample (with replacement) of `r bdg_mod$tuned$next.learner$par.vals$sample.fraction` of the number of units in
the training sample. The predictors used in each tree are also a random sample,
    where the number of predictors retained is equal to the square root of the total
number of predictors in the post-feature selection DTM.

The only model hyper-parameter that is tuned performed is the proportion of
variables to retain via the feature selection step described above. We use
cross-validation on the combined feature selection and random forest procedures
to choose an optimal proportion of variables to retain.


```{r perm_importance, cache = TRUE, echo = FALSE}

vi_rngr <- function(mod){
  dv <- getTaskTargetNames(mod$task)
  data <- getTaskData(mod$task)[,c(dv,
                                   getFilteredFeatures(mod$trained))]
  ranger_mod <- ranger::ranger(as.formula(paste0(dv, "~ .")),
                               data = data, importance = "permutation",
                               sample.fraction = .9)
  vip::vi(ranger_mod) %>%
    mutate(Model = dv) %>%
    top_n(10, Importance)
}

vi_bdg <- vi_rngr(bdg_mod)
vi_agd <- vi_rngr(agd_mod )
vi_bid <- vi_rngr(bid_mod)
vi_cafr <- vi_rngr(cafr_mod)
vi_min <- vi_rngr(min_mod)
vi_rec <- vi_rngr(rec_mod)
```

To understand what aspects of the website are used when predicting each label,
we computed the permutation importance of each column of the DTM. Specifically,
we permute each column of the DTM and calculate the increase in prediction
error. This statistic provides one measure of the relative importance of each
unigram count from the website. The results for the 6 models can be found in
Figure \@ref(fig:plot-perm-impt). Unsurprisingly, the most predictive unigrams
tend to be the labels themselves, such as "budgets", "minutes", etc.


```{r plot-perm-impt, layout = "l-page", fig.width = 8, fig.height = 6, echo=FALSE, fig.cap = "Permutation Importance. Shows 10 columns of the document term matrix that have the highest permutation importance."}

bind_rows(vi_agd, vi_bdg, vi_bid, vi_cafr,
          vi_min, vi_rec) %>%
  ggplot(aes(x = Importance,
             y = drlib::reorder_within(Variable, Importance, Model))) +
         geom_point() +
         drlib::scale_y_reordered() +
  facet_wrap(~ Model, ncol = 2, scales = "free") +
  theme_minimal() +
  ylab("Unigram") +
  xlab("Permutation Importance")
```



## Out of Sample Performance
```{r gather_data, echo = FALSE}

get_perf <- function(errors, label){
  tibble(label = label, 
         Measure = names(errors$cv_error$aggr),
         Performance = errors$cv_error$aggr)  
}
performance <- bind_rows(get_perf(bdg_errors, "BDG"),
                         get_perf(min_errors, "MIN"),
                         get_perf(rec_errors, "REC"),
                         get_perf(agd_errors, "AGD"),
                         get_perf(bid_errors, "BID"),
                         get_perf(cafr_errors, "CAFR")) %>%
  mutate(label = fct_reorder(label, Performance, .fun = mean),
         Measure = case_when(
           Measure == "acc.test.mean" ~ "Accuracy",
           Measure == "npv.test.mean" ~ "Negative Predictive Value",
           Measure == "ppv.test.mean" ~ "Positive Predictive Value"
         ))
```

To estimate performance of the model, we use cross-validation. Specifically, we
do 5-fold cross-validation, repeated 10 times. Table
\@ref(tab:performance-table) reports the results for overall accuracy, positive
predictive value, and negative predictive value. Positive predictive value is
defined as the proportion of correct labels among sites labeled as having the
indicator, while negative predictive value is the proportion of correct labels
among sites labeled as not having the indicator.

```{r performance-table, echo = FALSE}
spread(performance, key = Measure, value = Performance) %>%
  kable(digits = 2, caption = "Estimates of Out of Sample Error Using Cross-Validation") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```
